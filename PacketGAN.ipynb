{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'networks' from '/home/jaywalker/MachineLearning/PacketGAN/networks.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup imports and make ure the files we care about exist\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "from pprint import pprint\n",
    "\n",
    "#custom modules\n",
    "import feature_extraction\n",
    "import unsw_nb15_dataset\n",
    "import networks\n",
    "\n",
    "from importlib import reload #allow us to reload custom modules any time we like with modifications\n",
    "reload(feature_extraction)\n",
    "reload(unsw_nb15_dataset)\n",
    "reload(networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network input data shape:  torch.Size([64, 25, 137])\n",
      "Number of training examples:  2613\n"
     ]
    }
   ],
   "source": [
    "#set up the dataset(s)\n",
    "\n",
    "#set some hyperparameters\n",
    "sequence_length=25\n",
    "batch_size=64 #always using size 1 because I can't seem to make the GRU work correctly with larger batches\n",
    "\n",
    "#first, load the dataset\n",
    "data_set = unsw_nb15_dataset.UNSW_NB15('UNSW_NB15_full_clean.csv',\n",
    "                                       sequence_length=sequence_length, \n",
    "                                       transform=feature_extraction.build_feature_sequence_tensor)\n",
    "\n",
    "#pick out the attack examples\n",
    "data_set.use_only_category('DoS'); #in this case just look at DoS attack samples \n",
    "\n",
    "#get rid of this when you want to use the whole set\n",
    "fraction_to_use = 0.05\n",
    "data_set, _ = random_split(data_set, [round(len(data_set) * fraction_to_use), len(data_set) - round(len(data_set) * fraction_to_use)])\n",
    "\n",
    "train_set_length = round(len(data_set) * 0.8)\n",
    "test_set_length = len(data_set) - train_set_length\n",
    "train_set, test_set = random_split(data_set, [train_set_length, test_set_length])\n",
    "\n",
    "data_loader = DataLoader(train_set, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=2)\n",
    "\n",
    "data_example = None\n",
    "for count, data in enumerate(data_loader):\n",
    "    data_example = data\n",
    "    if count == 0:\n",
    "        break\n",
    "        \n",
    "print(\"Network input data shape: \", data_example.shape)\n",
    "print(\"Number of training examples: \", len(data_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features:  137\n",
      "Latent dimensions:  25\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train a KMeans classifier for cluster evaluation\n",
    "import evaluation\n",
    "reload(evaluation)\n",
    "\n",
    "num_features = data_set[0].shape[1]\n",
    "print(\"Number of input features: \", num_features)\n",
    "latent_dimensions = 25\n",
    "print(\"Latent dimensions: \", latent_dimensions)\n",
    "\n",
    "latent_space_mapper = networks.GRUMapping(num_features, latent_dimensions, batch_size)\n",
    "\n",
    "#convert the train_set and test_set to a lower dimensional space for clustering\n",
    "train_set_latent = []\n",
    "train_comparison_set, _ = random_split(train_set, [len(test_set), len(train_set) - len(test_set)])\n",
    "train_comparison_set_dl = torch.utils.data.DataLoader(train_comparison_set, batch_size=batch_size, shuffle=True) #use a dataloader so the examples are batched\n",
    "with torch.no_grad():\n",
    "    for example in train_comparison_set_dl:\n",
    "        if example.shape[0] != batch_size:\n",
    "            continue #ignore batches that don't fit the network so we don't have to deal with too many technicalities...\n",
    "            #maybe we could deal with that by changing the size of the hidden state of the network before inputting the data?\n",
    "        train_set_latent.extend(latent_space_mapper(example).numpy()[0])\n",
    "        \n",
    "#print(train_set_latent)\n",
    "\n",
    "test_set_latent = []\n",
    "test_set_dl = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "with torch.no_grad():\n",
    "    for example in test_set_dl:\n",
    "        if example.shape[0] != batch_size:\n",
    "            continue #ignore batches that don't fit the network to keep things easy\n",
    "        test_set_latent.extend(latent_space_mapper(example).numpy()[0])\n",
    "\n",
    "#print(\"Latent test set length: \", len(test_set_latent))\n",
    "#pprint(test_set_latent)\n",
    "#print(\"Latent test set element size: \", test_set_latent[0].shape)\n",
    "kmeans = evaluation.KMeansTorch(n_clusters=10)\n",
    "kmeans.fit(train_set_latent)\n",
    "\n",
    "#are the train set and test set from the same distribution?\n",
    "train_counts, test_counts = evaluation.cluster_and_compare(kmeans, train_set_latent, test_set_latent, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the networks \n",
    "G = networks.Generator(sequence_length, data_example.shape[2], data_example.shape[2] * 2, 30, data_example.shape[2], batch_size, torch.sigmoid, noise_input=True)\n",
    "D = networks.Discriminator(sequence_length, data_example.shape[2], data_example.shape[2] * 2, 1, batch_size, torch.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [------------------------------] 0.5%\n",
      "Epoch:  1 / 1\n",
      "D Real Error:  0.26645028591156006\n",
      "D Fake Error:  0.25168389081954956\n",
      "G Error:  1.5283095836639404\n",
      "Remaining time: 1:27:19.159004\n"
     ]
    }
   ],
   "source": [
    "#train GAN to replace masked packets\n",
    "#by iterating over the dataset for some # of epochs\n",
    "\n",
    "import train\n",
    "reload(train)\n",
    "reload(networks)\n",
    "\n",
    "#Train the network!\n",
    "num_epochs = 1\n",
    "G, D, g_losses, df_losses, g_stats, df_stats = train.train_gan(G,D,data_loader,num_epochs)\n",
    "\n",
    "#plot instantaneous losses\n",
    "plt.plot(range(len(df_losses)), df_losses, 'r-', range(len(g_losses)), g_losses, 'g-')\n",
    "plt.show()\n",
    "\n",
    "#plot loss averages\n",
    "plt.plot(range(len(df_stats.get_averages())), df_stats.get_averages(), 'r-', range(len(g_stats.get_averages())), g_stats.get_averages(), 'g-')\n",
    "plt.show()\n",
    "\n",
    "#cluster and compare. Are the fake examples even remotely matching the original distribution?\n",
    "#first generate a set of masked sequences\n",
    "gan_latent_test_set = []\n",
    "with torch.no_grad():\n",
    "    for example in test_set_dl:\n",
    "        if example.shape[0] != batch_size:\n",
    "            continue #ignore batches that don't fit the network to keep things easy\n",
    "        generated_example = G(example)\n",
    "        gan_latent_test_set.extend(latent_space_mapper(generated_example).numpy()[0])\n",
    "\n",
    "#then compare the native GAN output\n",
    "train_counts, test_counts = evaluation.cluster_and_compare(kmeans, train_set_latent, gan_latent_test_set, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a test of the wasserstein critic/approximate wasserstein distance\n",
    "import torch.utils.data\n",
    "import evaluation\n",
    "reload(evaluation)\n",
    "\n",
    "for k in range(0, 1):\n",
    "    real_set, generator_input, _ = random_split(test_set, [len(test_set) // 2, len(test_set) // 2, len(test_set) - 2*(len(test_set) // 2)]) #I'm assuming it's got at least 200 examples to spare\n",
    "    real_list = []\n",
    "    #assuming real_train_set and real_test_set are equal length\n",
    "    for i in range(0, len(real_set)):\n",
    "        real_list.append(feature_extraction.batchify([real_set[i]]))\n",
    "        \n",
    "    fake_set = []\n",
    "    for i in range(0, len(generator_input)):\n",
    "        #for this example I'll just use the raw generator output. We can mask later.\n",
    "        #this way it's consistent with the histogram above.\n",
    "        with torch.no_grad():\n",
    "            fake_set.append(G(feature_extraction.batchify([generator_input[i]]))) #G ouput already has batch dimension\n",
    "            \n",
    "    #print(len(real_set))\n",
    "    #print(real_list[0].shape)\n",
    "    #print(fake_set[0].shape)\n",
    "            \n",
    "    w_hat, accuracy = evaluation.wasserstein_critic(real_list, fake_set)\n",
    "    print(\"Wasserstein Distance: \", w_hat)\n",
    "    print(\"Classification Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_dict_compare(dict1, dict2):\n",
    "    for k in dict1:\n",
    "        if k not in dict2:\n",
    "            return None\n",
    "        \n",
    "    for k in dict1:\n",
    "        print(dict1[k], \"\\t\\t\", dict2[k])\n",
    "\n",
    "def dict_list_compare(dict_list1, dict_list2):\n",
    "    for l in range(0, len(dict_list1)):\n",
    "        print_dict_compare(dict_list1[l], dict_list2[l])\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "            \n",
    "import random\n",
    "data_example = None\n",
    "for count, data in enumerate(data_loader):\n",
    "    data_example = data\n",
    "    if count == 0:\n",
    "        break\n",
    "        \n",
    "print(\"Real data\\tFake data\")\n",
    "real_data = feature_extraction.decode_feature_sequence_tensor(data_set, data_example[0])\n",
    "\n",
    "#note here that \"gen\" is the trained generator returned from the training function above\n",
    "generated_data = G(data_example).detach()\n",
    "fake_data = feature_extraction.decode_feature_sequence_tensor(data_set, generated_data[0])\n",
    "\n",
    "dict_list_compare(real_data, fake_data)\n",
    "\n",
    "#print(\"From data set:\")\n",
    "#print(feature_extraction.decode_feature_sequence_tensor(data_set, data_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
