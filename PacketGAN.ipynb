{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'networks' from '/home/jaywalker/MachineLearning/PacketGAN/networks.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup imports and make ure the files we care about exist\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "#custom modules\n",
    "import feature_extraction\n",
    "import unsw_nb15_dataset\n",
    "import generator_discriminator\n",
    "import networks\n",
    "\n",
    "from importlib import reload #allow us to reload custom modules any time we like with modifications\n",
    "reload(feature_extraction)\n",
    "reload(unsw_nb15_dataset)\n",
    "reload(generator_discriminator)\n",
    "reload(networks)\n",
    "\n",
    "#fe.test_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network input data shape:  torch.Size([1, 25, 275])\n",
      "Number of training examples:  163\n"
     ]
    }
   ],
   "source": [
    "#set up the dataset(s)\n",
    "\n",
    "#set some hyperparameters\n",
    "sequence_length=25\n",
    "batch_size=1 #always using size 1 because I can't seem to make the GRU work correctly with larger batches\n",
    "\n",
    "#first, load the dataset\n",
    "data_set = unsw_nb15_dataset.UNSW_NB15('UNSW_NB15_full_clean.csv',\n",
    "                                       sequence_length=sequence_length, \n",
    "                                       transform=feature_extraction.build_feature_sequence_tensor)\n",
    "\n",
    "#pick out the attack examples\n",
    "data_set.use_only_category('DoS'); #in this case just look at DoS attack samples \n",
    "\n",
    "#TODO FIXME if I select a fraction >= 0.5 then the excluded set has no elements, can't use evaluation func\n",
    "train_set_length = round(len(data_set) * 0.01)\n",
    "test_set_length = train_set_length\n",
    "excluded_set_length = len(data_set) - train_set_length - test_set_length\n",
    "train_set, test_set, excluded_set = random_split(data_set, [train_set_length, test_set_length, excluded_set_length])\n",
    "\n",
    "data_loader = DataLoader(train_set, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=2)\n",
    "\n",
    "data_example = None\n",
    "for count, data in enumerate(data_loader):\n",
    "    data_example = data\n",
    "    if count == 0:\n",
    "        break\n",
    "        \n",
    "print(\"Network input data shape: \", data_example.shape)\n",
    "print(\"Number of training examples: \", len(data_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features:  275\n",
      "Latent dimensions:  25\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train a KMeans classifier for cluster evaluation\n",
    "import evaluation\n",
    "reload(evaluation)\n",
    "\n",
    "num_features = data_set[0].shape[1]\n",
    "print(\"Number of input features: \", num_features)\n",
    "latent_dimensions = 25\n",
    "print(\"Latent dimensions: \", latent_dimensions)\n",
    "\n",
    "latent_space_mapper = networks.GRUMapping(num_features, latent_dimensions)\n",
    "\n",
    "#convert the train_set and test_set to a lower dimensional space for clustering\n",
    "train_set_latent = []\n",
    "for example in train_set:\n",
    "    example = feature_extraction.batchify([example])\n",
    "    train_set_latent.append(latent_space_mapper(example).detach().numpy())\n",
    "\n",
    "test_set_latent = []\n",
    "for example in test_set:\n",
    "    example = feature_extraction.batchify([example])\n",
    "    test_set_latent.append(latent_space_mapper(example).detach().numpy())\n",
    "\n",
    "kmeans = evaluation.KMeansTorch(n_clusters=10)\n",
    "kmeans.fit(train_set_latent)\n",
    "\n",
    "#are the train set and test set from the same distribution?\n",
    "train_counts, test_counts = evaluation.clusterAndCompare(kmeans, train_set_latent, test_set_latent, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the networks\n",
    "        \n",
    "G = generator_discriminator.Generator(sequence_length, data_example.shape[2], data_example.shape[2] * 2, 30, data_example.shape[2], batch_size, torch.sigmoid)\n",
    "D = generator_discriminator.Discriminator(sequence_length, data_example.shape[2], data_example.shape[2] * 2, 1, batch_size, torch.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [###---------------------------] 9.8%\n",
      "Epoch:  1 / 2\n",
      "D Real Error:  0.03604651987552643\n",
      "D Fake Error:  0.03270108997821808\n",
      "G Error:  3.6563427448272705\n",
      "Remaining time: 0:09:17.579374\n"
     ]
    }
   ],
   "source": [
    "#train GAN to replace masked packets\n",
    "#by iterating over the dataset for some # of epochs\n",
    "\n",
    "import train\n",
    "reload(train)\n",
    "reload(networks)\n",
    "\n",
    "#Train the network!\n",
    "num_epochs = 2\n",
    "G, D, g_losses, df_losses, g_stats, df_stats = train.train_gan(G,D,data_loader,num_epochs)\n",
    "\n",
    "#plot instantaneous losses\n",
    "plt.plot(range(len(df_losses)), df_losses, 'r-', range(len(g_losses)), g_losses, 'g-')\n",
    "plt.show()\n",
    "\n",
    "#plot loss averages\n",
    "plt.plot(range(len(df_stats.get_averages())), df_stats.get_averages(), 'r-', range(len(g_stats.get_averages())), g_stats.get_averages(), 'g-')\n",
    "plt.show()\n",
    "\n",
    "#cluster and compare. Are the fake examples even remotely matching the original distribution?\n",
    "#first generate a set of masked sequences\n",
    "masked_gan_latent_test_set = []\n",
    "for example in test_set:\n",
    "    example = feature_extraction.batchify([example])\n",
    "    generated_example = G(example).detach()\n",
    "    #print(generated_example.shape)\n",
    "    #masked_example = train.get_interleaved_sequence_by_mask(in_example, generated_example, train.get_mask_vector(sequence_length, 0.2))\n",
    "    masked_example = generated_example\n",
    "    masked_gan_latent_test_set.append(latent_space_mapper(masked_example).detach().numpy())\n",
    "\n",
    "#then compare the native GAN output\n",
    "train_counts, test_counts = evaluation.clusterAndCompare(kmeans, train_set_latent, masked_gan_latent_test_set, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a test of the wasserstein critic/approximate wasserstein distance\n",
    "import torch.utils.data\n",
    "import evaluation\n",
    "reload(evaluation)\n",
    "\n",
    "for k in range(0, 1):\n",
    "    set_length = 250\n",
    "    second_excluded_set_length = len(excluded_set) - set_length * 2\n",
    "    real_set, generator_input, second_excluded_set = random_split(excluded_set, [set_length, set_length, second_excluded_set_length]) #I'm assuming it's got at least 200 examples to spare\n",
    "    real_list = []\n",
    "    #assuming real_train_set and real_test_set are equal length\n",
    "    for i in range(0, len(real_set)):\n",
    "        real_list.append(feature_extraction.batchify([real_set[i]]))\n",
    "        \n",
    "    fake_set = []\n",
    "    for i in range(0, len(generator_input)):\n",
    "        #for this example I'll just use the raw generator output. We can mask later.\n",
    "        #this way it's consistent with the histogram above.\n",
    "        with torch.no_grad():\n",
    "            fake_set.append(G(feature_extraction.batchify([generator_input[i]]))) #G ouput already has batch dimension\n",
    "            \n",
    "    #print(len(real_set))\n",
    "    #print(real_list[0].shape)\n",
    "    #print(fake_set[0].shape)\n",
    "            \n",
    "    w_hat, accuracy = evaluation.wasserstein_critic(real_list, fake_set)\n",
    "    print(\"Wasserstein Distance: \", w_hat)\n",
    "    print(\"Classification Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_dict_compare(dict1, dict2):\n",
    "    for k in dict1:\n",
    "        if k not in dict2:\n",
    "            return None\n",
    "        \n",
    "    for k in dict1:\n",
    "        print(dict1[k], \"\\t\\t\", dict2[k])\n",
    "\n",
    "def dict_list_compare(dict_list1, dict_list2):\n",
    "    for l in range(0, len(dict_list1)):\n",
    "        print_dict_compare(dict_list1[l], dict_list2[l])\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "            \n",
    "import random\n",
    "data_example = None\n",
    "for count, data in enumerate(data_loader):\n",
    "    data_example = data\n",
    "    if count == 0:\n",
    "        break\n",
    "        \n",
    "print(\"Real data\\tFake data\")\n",
    "real_data = feature_extraction.decode_feature_sequence_tensor(data_set, data_example[0])\n",
    "\n",
    "#note here that \"gen\" is the trained generator returned from the training function above\n",
    "generated_data = G(data_example).detach()\n",
    "fake_data = feature_extraction.decode_feature_sequence_tensor(data_set, generated_data[0])\n",
    "\n",
    "dict_list_compare(real_data, fake_data)\n",
    "\n",
    "#print(\"From data set:\")\n",
    "#print(feature_extraction.decode_feature_sequence_tensor(data_set, data_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
