{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'networks' from '/home/jaywalker/MachineLearning/PacketGAN/networks.py'>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup imports and make ure the files we care about exist\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "#custom modules\n",
    "import feature_extraction\n",
    "import unsw_nb15_dataset\n",
    "import generator_discriminator\n",
    "import networks\n",
    "\n",
    "from importlib import reload #allow us to reload custom modules any time we like with modifications\n",
    "reload(feature_extraction)\n",
    "reload(unsw_nb15_dataset)\n",
    "reload(generator_discriminator)\n",
    "reload(networks)\n",
    "\n",
    "#fe.test_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network input data shape:  torch.Size([1, 25, 275])\n"
     ]
    }
   ],
   "source": [
    "#set up the dataset(s)\n",
    "\n",
    "#set some hyperparameters\n",
    "sequence_length=25\n",
    "batch_size=1\n",
    "\n",
    "#first, load the dataset\n",
    "data_set = unsw_nb15_dataset.UNSW_NB15('/home/jaywalker/MachineLearning/PacketGAN/UNSW_NB15_full_clean.csv',\n",
    "                                       sequence_length=sequence_length, \n",
    "                                       transform=feature_extraction.build_feature_sequence_tensor)\n",
    "\n",
    "#pick out the attack examples\n",
    "data_set.use_only_category('DoS'); #in this case just look at DoS attack samples \n",
    "\n",
    "train_set_length = round(len(data_set) * 0.1)\n",
    "test_set_length = train_set_length\n",
    "excluded_set_length = len(data_set) - train_set_length - test_set_length\n",
    "train_set, test_set, excluded_set = random_split(data_set, [train_set_length, test_set_length, excluded_set_length])\n",
    "\n",
    "data_loader = DataLoader(train_set, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=2)\n",
    "\n",
    "data_example = None\n",
    "for count, data in enumerate(data_loader):\n",
    "    data_example = data\n",
    "    if count == 0:\n",
    "        break\n",
    "        \n",
    "print(\"Network input data shape: \", data_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features:  275\n",
      "Latent dimensions:  25\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train a KMeans classifier for cluster evaluation\n",
    "import evaluation\n",
    "reload(evaluation)\n",
    "\n",
    "num_features = data_set[0].shape[1]\n",
    "print(\"Number of input features: \", num_features)\n",
    "latent_dimensions = 25\n",
    "print(\"Latent dimensions: \", latent_dimensions)\n",
    "\n",
    "#TODO FIXME write a \"batchify\" function so that ALL networks receive (and process) examples\n",
    "#with a batch dimension (don't want some with/some without)\n",
    "\n",
    "latent_space_mapper = networks.GRUMapping(num_features, latent_dimensions)\n",
    "\n",
    "#convert the train_set and test_set to a lower dimensional space for clustering\n",
    "train_set_latent = []\n",
    "for example in train_set:\n",
    "    example = feature_extraction.batchify([example])\n",
    "    train_set_latent.append(latent_space_mapper(example).detach().numpy())\n",
    "\n",
    "test_set_latent = []\n",
    "for example in test_set:\n",
    "    example = feature_extraction.batchify([example])\n",
    "    test_set_latent.append(latent_space_mapper(example).detach().numpy())\n",
    "\n",
    "kmeans = evaluation.KMeansTorch(n_clusters=10)\n",
    "kmeans.fit(train_set_latent)\n",
    "\n",
    "#are the train set and test set from the same distribution?\n",
    "train_counts, test_counts = evaluation.clusterAndCompare(kmeans, train_set_latent, test_set_latent, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the networks\n",
    "        \n",
    "G = generator_discriminator.Generator(sequence_length, data_example.shape[2], data_example.shape[2] * 2, 30, data_example.shape[2], batch_size, torch.sigmoid)\n",
    "D = generator_discriminator.Discriminator(sequence_length, data_example.shape[2], data_example.shape[2] * 2, 1, batch_size, torch.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started...\n"
     ]
    }
   ],
   "source": [
    "#train GAN to replace masked packets\n",
    "#by iterating over the dataset for some # of epochs\n",
    "\n",
    "import train\n",
    "reload(train)\n",
    "reload(networks)\n",
    "\n",
    "#Train the network!\n",
    "num_epochs = 2\n",
    "G, D, g_losses, df_losses, g_stats, df_stats = train.train_gan(G,D,data_loader,num_epochs)\n",
    "\n",
    "#plot instantaneous losses\n",
    "plt.plot(range(len(df_losses)), df_losses, 'r-', range(len(g_losses)), g_losses, 'g-')\n",
    "plt.show()\n",
    "\n",
    "#plot loss averages\n",
    "plt.plot(range(len(df_stats.get_averages())), df_stats.get_averages(), 'r-', range(len(g_stats.get_averages())), g_stats.get_averages(), 'g-')\n",
    "plt.show()\n",
    "\n",
    "#cluster and compare. Are the fake examples even remotely matching the original distribution?\n",
    "#first generate a set of masked sequences\n",
    "masked_gan_latent_test_set = []\n",
    "for example in test_set:\n",
    "    example = feature_extraction.batchify([example])\n",
    "    generated_example = G(example).detach()\n",
    "    #print(generated_example.shape)\n",
    "    #masked_example = train.get_interleaved_sequence_by_mask(in_example, generated_example, train.get_mask_vector(sequence_length, 0.2))\n",
    "    masked_example = generated_example\n",
    "    masked_gan_latent_test_set.append(latent_space_mapper(masked_example).detach().numpy())\n",
    "\n",
    "#then compare the native GAN output\n",
    "train_counts, test_counts = evaluation.clusterAndCompare(kmeans, train_set_latent, masked_gan_latent_test_set, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_dict_compare(dict1, dict2):\n",
    "    for k in dict1:\n",
    "        if k not in dict2:\n",
    "            return None\n",
    "        \n",
    "    for k in dict1:\n",
    "        print(dict1[k], \"\\t\\t\", dict2[k])\n",
    "\n",
    "def dict_list_compare(dict_list1, dict_list2):\n",
    "    for l in range(0, len(dict_list1)):\n",
    "        print_dict_compare(dict_list1[l], dict_list2[l])\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "            \n",
    "import random\n",
    "data_example = None\n",
    "for count, data in enumerate(data_loader):\n",
    "    data_example = data\n",
    "    if count == 0:\n",
    "        break\n",
    "        \n",
    "print(\"Real data\\tFake data\")\n",
    "real_data = feature_extraction.decode_feature_sequence_tensor(data_set, data_example[0])\n",
    "\n",
    "#note here that \"gen\" is the trained generator returned from the training function above\n",
    "generated_data = G(data_example).detach()\n",
    "fake_data = feature_extraction.decode_feature_sequence_tensor(data_set, generated_data[0])\n",
    "\n",
    "dict_list_compare(real_data, fake_data)\n",
    "\n",
    "#print(\"From data set:\")\n",
    "#print(feature_extraction.decode_feature_sequence_tensor(data_set, data_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
