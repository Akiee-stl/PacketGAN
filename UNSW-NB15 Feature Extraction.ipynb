{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features file found!\n"
     ]
    }
   ],
   "source": [
    "#setup imports and make ure the files we care about exist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "features_path = \"/home/jaywalker/MachineLearning/UNSW-NB15-CSV/NUSW-NB15_features.csv\"\n",
    "print(\"Features file found!\" if os.path.isfile(\"/home/jaywalker/MachineLearning/UNSW-NB15-CSV/NUSW-NB15_features.csv\") else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['no.', 'name', 'type', 'description'], dtype='object')\n",
      "                name       type\n",
      "0              srcip    nominal\n",
      "1              sport    integer\n",
      "2              dstip    nominal\n",
      "3             dsport    integer\n",
      "4              proto    nominal\n",
      "5              state    nominal\n",
      "6                dur      float\n",
      "7             sbytes    integer\n",
      "8             dbytes    integer\n",
      "9               sttl    integer\n",
      "10              dttl    integer\n",
      "11             sloss    integer\n",
      "12             dloss    integer\n",
      "13           service    nominal\n",
      "14             sload      float\n",
      "15             dload      float\n",
      "16             spkts    integer\n",
      "17             dpkts    integer\n",
      "18              swin    integer\n",
      "19              dwin    integer\n",
      "20             stcpb    integer\n",
      "21             dtcpb    integer\n",
      "22           smeansz    integer\n",
      "23           dmeansz    integer\n",
      "24       trans_depth    integer\n",
      "25       res_bdy_len    integer\n",
      "26              sjit      float\n",
      "27              djit      float\n",
      "28             stime  timestamp\n",
      "29             ltime  timestamp\n",
      "30           sintpkt      float\n",
      "31           dintpkt      float\n",
      "32            tcprtt      float\n",
      "33            synack      float\n",
      "34            ackdat      float\n",
      "35   is_sm_ips_ports     binary\n",
      "36      ct_state_ttl    integer\n",
      "37  ct_flw_http_mthd    integer\n",
      "38      is_ftp_login     binary\n",
      "39        ct_ftp_cmd    integer\n",
      "40        ct_srv_src    integer\n",
      "41        ct_srv_dst    integer\n",
      "42        ct_dst_ltm    integer\n",
      "43       ct_src_ ltm    integer\n",
      "44  ct_src_dport_ltm    integer\n",
      "45  ct_dst_sport_ltm    integer\n",
      "46    ct_dst_src_ltm    integer\n",
      "47        attack_cat    nominal\n",
      "48             label     binary\n"
     ]
    }
   ],
   "source": [
    "#quick peek at the features and cleaning up column names for easier indexing\n",
    "features_df = pd.read_csv(features_path, encoding=\"latin-1\")\n",
    "for i in range(len(features_df.columns.values)):\n",
    "    features_df.columns.values[i] = str(features_df.columns.values[i]).strip().lower()\n",
    "    \n",
    "    \n",
    "print(features_df.columns) #cleaned up column names\n",
    "\n",
    "#lower case all the types\n",
    "for i in range(len(features_df)):\n",
    "    features_df.loc[i, ['type']] = str(features_df['type'][i]).strip().lower()\n",
    "    features_df.loc[i, ['name']] = str(features_df['name'][i]).strip().lower()\n",
    "\n",
    "print(features_df[['name', 'type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id       dur proto service state  spkts  dpkts  sbytes  dbytes        rate  \\\n",
      "0   1  0.000011   udp       -   INT      2      0     496       0  90909.0902   \n",
      "\n",
      "   ...    ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  \\\n",
      "0  ...                   1               2             0           0   \n",
      "\n",
      "   ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  attack_cat  \\\n",
      "0                 0           1           2                0      Normal   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "\n",
      "[1 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "#quick peek at the data\n",
    "training_set_path = \"/home/jaywalker/MachineLearning/UNSW-NB15-CSV/train_test/UNSW_NB15_training-set.csv\"\n",
    "training_df = pd.read_csv(training_set_path, encoding=\"latin-1\")\n",
    "print(training_df[:1])\n",
    "#Of COURSE this file is organized differently than the features file describes.\n",
    "#Why would I expect differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes',\n",
      "       'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service', 'sload', 'dload',\n",
      "       'spkts', 'dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz',\n",
      "       'dmeansz', 'trans_depth', 'res_bdy_len', 'sjit', 'djit', 'stime',\n",
      "       'ltime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
      "       'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login',\n",
      "       'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ ltm',\n",
      "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#so we'll use a file from the \"full\" dataset instead\n",
    "nb15_1_path = \"/home/jaywalker/MachineLearning/PacketGAN/UNSW-NB15_1_clean.csv\"\n",
    "dtypes = {\"sport\": \"int32\", \"dsport\": \"int32\"}\n",
    "packet_data_df = pd.read_csv(nb15_1_path, encoding=\"latin-1\", names=features_df['name'], header=None, dtype=dtypes)\n",
    "print(packet_data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcip: ['175.45.176.0', '175.45.176.1', '175.45.176.2', '175.45.176.3'] type: nominal\n",
      "sport: 9607 type: integer\n",
      "dstip: ['149.171.126.15', '149.171.126.14', '149.171.126.10', '149.171.126.13', '149.171.126.18', '149.171.126.19', '149.171.126.11', '149.171.126.17', '149.171.126.12', '149.171.126.16'] type: nominal\n",
      "dsport: 786 type: integer\n",
      "proto: 129 type: nominal\n",
      "state: ['INT', 'CON', 'REQ', 'FIN', 'ACC', 'CLO'] type: nominal\n",
      "dur: 8748 type: float\n",
      "sbytes: 2604 type: integer\n",
      "dbytes: 2410 type: integer\n",
      "sttl: [254, 0, 62, 255, 63] type: integer\n",
      "dttl: [0, 60, 252, 253] type: integer\n",
      "sloss: 186 type: integer\n",
      "dloss: 200 type: integer\n",
      "service: ['-', 'dhcp', 'dns', 'http', 'ftp-data', 'smtp', 'pop3', 'ftp', 'snmp', 'radius', 'ssl', 'ssh', 'irc'] type: nominal\n",
      "sload: 9501 type: float\n",
      "dload: 8654 type: float\n",
      "spkts: 225 type: integer\n",
      "dpkts: 245 type: integer\n",
      "swin: [0, 255] type: integer\n",
      "dwin: [0, 255] type: integer\n",
      "stcpb: 8584 type: integer\n",
      "dtcpb: 8552 type: integer\n",
      "smeansz: 1072 type: integer\n",
      "dmeansz: 986 type: integer\n",
      "trans_depth: [0, 1, 2, 8, 4, 3] type: integer\n",
      "res_bdy_len: 492 type: integer\n",
      "sjit: 8678 type: float\n",
      "djit: 8552 type: float\n",
      "stime: 6490 type: timestamp\n",
      "ltime: 6081 type: timestamp\n",
      "sintpkt: 8719 type: float\n",
      "dintpkt: 8581 type: float\n",
      "tcprtt: 8247 type: float\n",
      "synack: 8110 type: float\n",
      "ackdat: 7970 type: float\n",
      "is_sm_ips_ports: [0] type: binary\n",
      "ct_state_ttl: [2, 3, 6, 1, 0, 4, 5] type: integer\n",
      "ct_flw_http_mthd: [0, 1, 2] type: integer\n",
      "is_ftp_login: [0, 1] type: binary\n",
      "ct_ftp_cmd: [0, 1] type: integer\n",
      "ct_srv_src: 33 type: integer\n",
      "ct_srv_dst: 31 type: integer\n",
      "ct_dst_ltm: [2, 3, 1, 4, 6, 5, 14, 7, 8, 9, 11, 16, 10, 12, 13, 15] type: integer\n",
      "ct_src_ ltm: [1, 3, 4, 2, 5, 15, 6, 7, 8, 9, 13, 11, 10, 12, 14, 16, 17] type: integer\n",
      "ct_src_dport_ltm: [1, 2, 5, 3, 4, 14, 6, 8, 7, 11, 15, 10, 16, 12, 9, 13] type: integer\n",
      "ct_dst_sport_ltm: [1, 2, 4, 14, 6, 8, 3, 5] type: integer\n",
      "ct_dst_src_ltm: [1, 2, 3, 4, 14, 6, 8, 5, 7, 11, 9, 16, 10, 12, 13, 15] type: integer\n",
      "attack_cat: ['Reconnaissance', 'Exploits', 'Generic', ' Fuzzers', 'DoS', 'Backdoors', 'Analysis', 'Shellcode', 'Worms'] type: nominal\n",
      "label: [1] type: binary\n",
      "Normal packets:  677786\n",
      "Attack packets:  22215\n"
     ]
    }
   ],
   "source": [
    "#for each feature of type \"nominal\" or \"integer\" count how many classes exist\n",
    "#print(packet_data_df['label'].unique()) #identify the different values\n",
    "\n",
    "for label, feature_type in features_df[['name', 'type']].values:\n",
    "    nunique = packet_data_df[packet_data_df['label'] == 1][label].nunique()\n",
    "    if nunique < 20:\n",
    "        value_list = packet_data_df[packet_data_df['label'] == 1][label].unique().tolist()\n",
    "        print(label + \": \" , end='')\n",
    "        print(value_list, end='')\n",
    "        print(\" type: \" + str(feature_type))\n",
    "    else:\n",
    "        print(label + \": \" + str(nunique) + \" type: \" + str(feature_type))\n",
    "        \n",
    "        \n",
    "#how many attack packets do we have compared to non-attack packets?\n",
    "print(\"Normal packets: \", len(packet_data_df[packet_data_df['label'] == 0].index))\n",
    "print(\"Attack packets: \", len(packet_data_df[packet_data_df['label'] == 1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we convert the columns to the appropriate type, should just load them with the right\n",
    "#data type to start\n",
    "for k in range(0, packet_data_df['sport'].shape[0]):\n",
    "    if (isinstance(packet_data_df.loc[k, 'sport'], str)):\n",
    "        packet_data_df.loc[k, 'sport'] = int(packet_data_df.loc[k, 'sport'])\n",
    "        \n",
    "#did we convert all the strings?\n",
    "for k in range(0, packet_data_df['sport'].shape[0]):\n",
    "    if (isinstance(packet_data_df.loc[k, 'sport'], str)):\n",
    "        print(packet_data_df.loc[k, 'sport'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1]\n",
      "None\n",
      "7\n",
      "13\n",
      "13\n",
      "Inversion test: \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "#How can we encode these various features, many of which are discrete integers?\n",
    "#One-hot or Binary encoding seems logical, using Binary coding to keep things compact.\n",
    "\n",
    "#Returns a list where each element are a 1 or 0, determining the binary encoding of value with\n",
    "#at least bits number of bits. If the value cannot be encoding with the requested number of bits,\n",
    "#None will be returned.\n",
    "def binary_encode(value, bits):\n",
    "    encoding = []\n",
    "    while value != 0:\n",
    "        encoding.append(value % 2)\n",
    "        value //= 2\n",
    "        \n",
    "    if bits < len(encoding):\n",
    "        return None #couldn't represent with requested number of bits\n",
    "    \n",
    "    while len(encoding) < bits:\n",
    "        encoding.append(0)\n",
    "    \n",
    "    encoding.reverse()\n",
    "    return encoding\n",
    "\n",
    "#Takes binary integer in the form of a list containing 1's and 0's. \n",
    "#Returns the base-10 (integer) representation of the binary value.\n",
    "def binary_decode(value):\n",
    "    if len(value) == 0:\n",
    "        return None\n",
    "    \n",
    "    out = 0\n",
    "    for i in range(0, len(value)):\n",
    "        if value[i] == 1:\n",
    "            out += 2**(len(value) - (i+1))\n",
    "            \n",
    "    return out\n",
    "\n",
    "def float_to_binary(value):\n",
    "    out = []\n",
    "    for i in range(len(value)):\n",
    "        if value[i] >= 0.5:\n",
    "            out.append(1)\n",
    "        else:\n",
    "            out.append(0)\n",
    "            \n",
    "    return out\n",
    "        \n",
    "print(binary_encode(7, 4)) #returns [0,1,1,1]\n",
    "print(binary_encode(255, 2)) #returns None\n",
    "\n",
    "print(binary_decode([0,1,1,1])) #returns 7\n",
    "print(binary_decode([1,1,0,1])) #returns 13\n",
    "print(binary_decode(float_to_binary([0.55, 0.98, 0.34, 0.6]))) #returns 13\n",
    "\n",
    "print(\"Inversion test: \")\n",
    "for i in range(0,16):\n",
    "    print(binary_decode(binary_encode(i, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#try converting an ip into a bit array\n",
    "ip_as_bits = []\n",
    "for byte in packet_data_df['srcip'][0].split('.'):\n",
    "    ip_as_bits += binary_encode(int(byte), 8)\n",
    "    \n",
    "print(ip_as_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1421927415\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#lets see what some of the other relevant fields look like\n",
    "timestamp = packet_data_df['stime'][0]\n",
    "print(timestamp)\n",
    "nbits = 36\n",
    "print(binary_encode(timestamp, nbits))\n",
    "#can all the timestamps be represented with fewer bits?\n",
    "for k in packet_data_df['stime']:\n",
    "    if binary_encode(k, nbits) is None:\n",
    "        print(\"Couldn't map all the timestamps!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name       type\n",
      "0              srcip    nominal\n",
      "1              sport    integer\n",
      "2              dstip    nominal\n",
      "3             dsport    integer\n",
      "4              proto    nominal\n",
      "5              state    nominal\n",
      "6                dur      float\n",
      "7             sbytes    integer\n",
      "8             dbytes    integer\n",
      "9               sttl    integer\n",
      "10              dttl    integer\n",
      "11             sloss    integer\n",
      "12             dloss    integer\n",
      "13           service    nominal\n",
      "14             sload      float\n",
      "15             dload      float\n",
      "16             spkts    integer\n",
      "17             dpkts    integer\n",
      "18              swin    integer\n",
      "19              dwin    integer\n",
      "20             stcpb    integer\n",
      "21             dtcpb    integer\n",
      "22           smeansz    integer\n",
      "23           dmeansz    integer\n",
      "24       trans_depth    integer\n",
      "25       res_bdy_len    integer\n",
      "26              sjit      float\n",
      "27              djit      float\n",
      "28             stime  timestamp\n",
      "29             ltime  timestamp\n",
      "30           sintpkt      float\n",
      "31           dintpkt      float\n",
      "32            tcprtt      float\n",
      "33            synack      float\n",
      "34            ackdat      float\n",
      "35   is_sm_ips_ports     binary\n",
      "36      ct_state_ttl    integer\n",
      "37  ct_flw_http_mthd    integer\n",
      "38      is_ftp_login     binary\n"
     ]
    }
   ],
   "source": [
    "#what features do I care about?\n",
    "#all the non-aggregate features that are some combination of the other features\n",
    "#either directly in an example or temporal combinations, since these should ostensibly\n",
    "#be discovered by the GAN\n",
    "features_to_use = features_df[:39]\n",
    "print(features_to_use[['name', 'type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature encoding\n",
    "-------------------------\n",
    "\n",
    "I will encode the integer based features using a binary representation, using the minimum number of bits to represent the max value plus one bit. Float based parameters will be scaled in a typical manner.\n",
    "\n",
    "IP addresses in particular are a special case, since each field is represending a collection of 4 bytes. These addresses will be represented as 32 bits, since this is the native representation and seems appropriate for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input values:  {'srcip': '10.40.170.2', 'sport': 0, 'dstip': '10.40.170.2', 'dsport': '0', 'proto': 'arp', 'state': 'INT', 'dur': 0.0, 'sbytes': 46, 'dbytes': 0, 'sttl': 0, 'dttl': 0, 'sloss': 0, 'dloss': 0, 'service': '-', 'sload': 0.0, 'dload': 0.0, 'spkts': 1, 'dpkts': 0, 'swin': 0, 'dwin': 0, 'stcpb': 0, 'dtcpb': 0, 'smeansz': 46, 'dmeansz': 0, 'trans_depth': 0, 'res_bdy_len': 0, 'sjit': 0.0, 'djit': 0.0, 'stime': 1421927415, 'ltime': 1421927415, 'sintpkt': 0.0, 'dintpkt': 0.0, 'tcprtt': 0.0, 'synack': 0.0, 'ackdat': 0.0, 'is_sm_ips_ports': 1, 'ct_state_ttl': 2, 'ct_flw_http_mthd': 0, 'is_ftp_login': 0, 'ct_ftp_cmd': 0, 'ct_srv_src': 2, 'ct_srv_dst': 2, 'ct_dst_ltm': 2, 'ct_src_ ltm': 2, 'ct_src_dport_ltm': 2, 'ct_dst_sport_ltm': 2, 'ct_dst_src_ltm': 2, 'attack_cat': nan, 'label': 0}\n",
      "Output values:  {'srcip': '10.40.170.2', 'dstip': '10.40.170.2', 'sport': 0, 'dport': 0}\n",
      "X_seq:  tensor([[[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "Decoded feature sequence:  [{'srcip': '10.40.170.2', 'dstip': '10.40.170.2', 'sport': 0, 'dport': 0}, {'srcip': '10.40.170.2', 'dstip': '10.40.170.2', 'sport': 0, 'dport': 0}, {'srcip': '10.40.170.2', 'dstip': '10.40.170.2', 'sport': 0, 'dport': 0}, {'srcip': '10.40.170.2', 'dstip': '10.40.170.2', 'sport': 0, 'dport': 0}, {'srcip': '10.40.170.2', 'dstip': '10.40.170.2', 'sport': 0, 'dport': 0}]\n"
     ]
    }
   ],
   "source": [
    "def get_minimum_bits(value):\n",
    "    min_bits = 1\n",
    "    while binary_encode(value, min_bits) is None:\n",
    "        min_bits += 1\n",
    "        \n",
    "    return min_bits\n",
    "\n",
    "def build_input_feature_tensor(packet_data_dict):\n",
    "    input_features = []\n",
    "    \n",
    "    srcip_segments = str(packet_data_dict['srcip']).split('.')\n",
    "    srcip_bits = []\n",
    "    for segment in srcip_segments:\n",
    "        for k in binary_encode(int(segment), 8):\n",
    "            srcip_bits.append(k)\n",
    "    \n",
    "    dstip_segments = str(packet_data_dict['dstip']).split('.')\n",
    "    dstip_bits = []\n",
    "    for segment in dstip_segments:\n",
    "        for k in binary_encode(int(segment), 8):\n",
    "            dstip_bits.append(k)\n",
    "            \n",
    "    sport = binary_encode(int(packet_data_dict['sport']), 16)#get_minimum_bits(int(packet_data_dict['sport'])) + 1)\n",
    "    dport = binary_encode(int(packet_data_dict['dsport']), 16)#get_minimum_bits(int(packet_data_dict['dsport'])) + 1)\n",
    "    \n",
    "    #TODO need to encode the rest of the features buuuuuttttt that can come later.\n",
    "    \n",
    "    input_features += srcip_bits + dstip_bits + sport + dport\n",
    "    \n",
    "    return torch.tensor(input_features, dtype=torch.float32)\n",
    "        \n",
    "#Revert a feature tensor to human readable form\n",
    "#This working correctly is heavily dependent on sizes and locations chosen in \n",
    "#build_input_feature_tensor()\n",
    "def decode_feature_tensor(feature_tensor):\n",
    "    output_values = {}\n",
    "    \n",
    "    srcip_segments = []\n",
    "    for i in [0,1,2,3]:\n",
    "        srcip_segments.append(binary_decode(float_to_binary(feature_tensor[i*8:(i*8)+8])))\n",
    "        \n",
    "    srcip_string = \".\".join([str(k) for k in srcip_segments])\n",
    "    \n",
    "    dstip_segments = []\n",
    "    for i in [4,5,6,7]:\n",
    "        dstip_segments.append(binary_decode(float_to_binary(feature_tensor[i*8:(i*8)+8])))\n",
    "        \n",
    "    dstip_string = \".\".join([str(k) for k in dstip_segments])\n",
    "    \n",
    "    sport = binary_decode(float_to_binary(feature_tensor[64:64+16]))\n",
    "    dport = binary_decode(float_to_binary(feature_tensor[64+16:64+16+16]))\n",
    "    \n",
    "    output_values['srcip'] = srcip_string\n",
    "    output_values['dstip'] = dstip_string\n",
    "    output_values['sport'] = sport\n",
    "    output_values['dport'] = dport\n",
    "    \n",
    "    return output_values\n",
    "    \n",
    "print(\"Input values: \", packet_data_df.loc[0,:].to_dict())\n",
    "X = build_input_feature_tensor(packet_data_df.loc[0,:].to_dict())\n",
    "Xreadable = decode_feature_tensor(X)\n",
    "print(\"Output values: \", Xreadable)\n",
    "\n",
    "#just playing with tensors here to figure out what I'm doing!\n",
    "#print(X)\n",
    "#print(X.shape)\n",
    "X_seq = torch.tensor(()).new_zeros([5,1,X.shape[0]])\n",
    "#print(X_seq.shape)\n",
    "\n",
    "#wow, I did not expect this to work!\n",
    "X_seq[:,0,:] = X\n",
    "#print(X_seq)\n",
    "\n",
    "def build_feature_sequence_tensor(packet_data_df, sequence_length):\n",
    "    example_feature_vector = build_input_feature_tensor(packet_data_df.loc[0,:].to_dict())\n",
    "    seq_out = torch.tensor(()).new_zeros([sequence_length, 1, example_feature_vector.shape[0]])\n",
    "    \n",
    "    for i in range(0, sequence_length):\n",
    "        #print(seq_out.shape)\n",
    "        seq_out[i,0,:] = build_input_feature_tensor(packet_data_df.loc[i,:].to_dict())\n",
    "        \n",
    "    return seq_out\n",
    "\n",
    "def decode_feature_sequence_tensor(sequence_tensor):\n",
    "    seq_out = []\n",
    "\n",
    "    for i in range(0, sequence_tensor.shape[0]):\n",
    "        seq_out.append(decode_feature_tensor(sequence_tensor[i,0,:]))\n",
    "        \n",
    "    return seq_out\n",
    "    \n",
    "X_seq = build_feature_sequence_tensor(packet_data_df, 5)\n",
    "\n",
    "print(\"X_seq: \", X_seq)\n",
    "print(\"Decoded feature sequence: \", decode_feature_sequence_tensor(X_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n",
      "tensor([[[1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.]]])\n",
      "tensor([[[1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "          0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "          0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "          0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "          0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "          0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "          1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "#Returns a binary vector that determines whether a particular packet \n",
    "#in a sequence will be replaced by the Generator's output.\n",
    "#Worth noting that we NEVER mask the first value.\n",
    "#0 means replace, 1 means leave alone.\n",
    "def get_mask_vector(length = None, eta = 0.2):\n",
    "    if length is None:\n",
    "        return None\n",
    "    \n",
    "    mask = []\n",
    "    for k in range(0, length):\n",
    "        if random.random() < eta:\n",
    "            mask.append(0)\n",
    "        else:\n",
    "            mask.append(1)\n",
    "            \n",
    "    mask[0] = 1\n",
    "    return mask\n",
    "\n",
    "print(get_mask_vector(5))\n",
    "\n",
    "#Returns a Tensor of the same shape as first_sequence and second_sequence\n",
    "#According the rule where elements Xi of the first sequence is replaced by\n",
    "#element Yi of the second_sequence if the entry Ki of mask is equal to 0.\n",
    "#The resulting vector X is returned.\n",
    "def get_interleaved_sequence_by_mask(first_sequence, second_sequence, mask):\n",
    "    assert (first_sequence.shape == second_sequence.shape),\"Sequences must have the same shape.\"\n",
    "    assert (first_sequence.shape[0] == len(mask)),\"Sequences and mask must have the same 1st dimension.\"\n",
    "    masked_sequence = first_sequence.copy_(first_sequence)\n",
    "    \n",
    "    for i in range(0, len(mask)):\n",
    "        if mask[i] == 0:\n",
    "            masked_sequence[i,0] = second_sequence[i,0]\n",
    "            \n",
    "    return masked_sequence\n",
    "\n",
    "ones_tensor = torch.ones(5,1,3)\n",
    "zeros_tensor = torch.zeros(5,1,3)\n",
    "print(get_interleaved_sequence_by_mask(ones_tensor, zeros_tensor, get_mask_vector(ones_tensor.shape[0], eta=0.5)))\n",
    "\n",
    "def get_random_feature_sequence_tensor(packet_df, sequence_length):\n",
    "    packet_df = packet_df.reset_index() #start indices at 0\n",
    "    \n",
    "    min_index = 0\n",
    "    max_index = len(packet_df.index) - sequence_length\n",
    "    \n",
    "    sequence_start = random.randint(min_index, max_index)\n",
    "    selected_sequence = packet_df.loc[sequence_start:sequence_start + sequence_length, :]\n",
    "    feature_sequence_tensor = build_feature_sequence_tensor(selected_sequence.reset_index(), sequence_length)\n",
    "    \n",
    "    return feature_sequence_tensor\n",
    "\n",
    "print(get_random_feature_sequence_tensor(packet_data_df[packet_data_df['label'] == 1], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I shouldn't actually do the training in this notebook, this is mostly a test to see if I've prepared\n",
    "#the features correctly for input to some RNN network.\n",
    "\n",
    "#MODELS: Define Generator model and Discriminator model\n",
    "#For the time being, this will be a one-layer RNN that is the same width as the input feature tensor\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, noise_size, output_size, f):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.noise_size = noise_size\n",
    "        self.h0 = self.init_hidden_state()\n",
    "        self.gru = nn.GRU(input_size + noise_size, self.hidden_size)\n",
    "        self.map = nn.Linear(self.hidden_size, input_size)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn(x.shape[0], x.shape[1], self.noise_size)\n",
    "        x = torch.cat((x, noise), 2)\n",
    "        x, self.h0 = self.gru(x, self.h0)\n",
    "        x = self.map(x)\n",
    "        return self.f(x)\n",
    "    \n",
    "    def init_hidden_state(self):\n",
    "        self.h0 = torch.zeros(1,1,self.hidden_size)\n",
    "        return self.h0\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h0 = self.init_hidden_state()\n",
    "        self.gru = nn.GRU(input_size, self.hidden_size)\n",
    "        self.map = nn.Linear(self.hidden_size, 1)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, self.h0 = self.gru(x, self.h0)\n",
    "        x = self.map(x)\n",
    "        return self.f(x)\n",
    "    \n",
    "    def init_hidden_state(self):\n",
    "        self.h0 = torch.zeros(1,1,self.hidden_size)\n",
    "        return self.h0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#Define Generator and Discriminator\n",
    "G = Generator(X_seq.shape[2], X_seq.shape[2], 25, X_seq.shape[2], torch.sigmoid)\n",
    "D = Discriminator(X_seq.shape[2], X_seq.shape[2], 1, torch.sigmoid)\n",
    "\n",
    "#testing how resetting the hidden state has an effect on the output\n",
    "G.zero_grad()\n",
    "G.init_hidden_state()\n",
    "first = G(torch.tensor(X_seq)).detach()\n",
    "#print(first)\n",
    "\n",
    "G.zero_grad()\n",
    "G.init_hidden_state()\n",
    "second = G(torch.tensor(X_seq)).detach()\n",
    "#print(second)\n",
    "\n",
    "#these outputs should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "D Real Error:  0.7440565824508667\n",
      "D Fake Error:  0.6645587682723999\n",
      "G Error:  0.711696982383728\n",
      "Epoch:  1000\n",
      "D Real Error:  0.33198967576026917\n",
      "D Fake Error:  0.18852505087852478\n",
      "G Error:  5.7178874015808105\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
    "\n",
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "#well OK let's train the GAN on a single sequence and see what happens\n",
    "def train(G, D, real_data):\n",
    "    num_epochs = 100000\n",
    "    print_interval = 1000\n",
    "    loss_log_interval = 100\n",
    "    print_stats = True\n",
    "    sequence_length = 20\n",
    "    eta = 0.2 #probability of masking an element of a sequence\n",
    "    \n",
    "    discriminator_learning_rate = 1e-3\n",
    "    generator_learning_rate = 1e-3\n",
    "    sgd_momentum = 0.9\n",
    "    \n",
    "    discriminator_training_steps = 1\n",
    "    generator_training_steps = 1\n",
    "    \n",
    "    discriminator_fake_error, discriminator_real_error = 0.0, 0.0\n",
    "    generator_error = 0\n",
    "    \n",
    "    generator_losses = []\n",
    "    discriminator_fake_losses = []\n",
    "    \n",
    "    criterion = nn.BCELoss() #right now the output is binary so this makes sense\n",
    "    discriminator_optimizer = optim.SGD(D.parameters(), lr=discriminator_learning_rate, momentum=sgd_momentum)\n",
    "    generator_optimizer = optim.SGD(G.parameters(), lr=discriminator_learning_rate, momentum=sgd_momentum)\n",
    "    for epoch in range(num_epochs):\n",
    "        for discriminator_step in range(discriminator_training_steps):\n",
    "            #G.init_hidden_state()\n",
    "            #G.zero_grad()\n",
    "            D.init_hidden_state()\n",
    "            D.zero_grad()\n",
    "            \n",
    "            #Train D on the real samples\n",
    "            #print(\"Sequence length: \", sequence_length )\n",
    "            discriminator_decision_r = D(get_random_feature_sequence_tensor(real_data, sequence_length))\n",
    "            discriminator_real_error = criterion(discriminator_decision_r, torch.ones(sequence_length))\n",
    "            discriminator_real_error.backward()\n",
    "            \n",
    "            #Train D on the fake samples\n",
    "            D.init_hidden_state()\n",
    "            \n",
    "            #get a real example and mask some of them with generator output\n",
    "            generator_input_sequence = get_random_feature_sequence_tensor(real_data, sequence_length)\n",
    "            fake_data = G(generator_input_sequence).detach()\n",
    "            fake_masked_data = get_interleaved_sequence_by_mask(generator_input_sequence, fake_data, get_mask_vector(sequence_length, eta))\n",
    "            \n",
    "            discriminator_decision_f = D(fake_masked_data)\n",
    "            discriminator_fake_error = criterion(discriminator_decision_f, torch.zeros(sequence_length))\n",
    "            discriminator_fake_error.backward()\n",
    "            discriminator_optimizer.step()\n",
    "            \n",
    "            dre = extract(discriminator_real_error)[0]\n",
    "            dfe = extract(discriminator_fake_error)[0]\n",
    "    \n",
    "        for generator_step in range(generator_training_steps):\n",
    "            #Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.init_hidden_state()\n",
    "            G.zero_grad()\n",
    "            D.init_hidden_state()\n",
    "            D.zero_grad()\n",
    "            \n",
    "            generator_input_sequence = get_random_feature_sequence_tensor(real_data, sequence_length)\n",
    "            fake_data = G(generator_input_sequence).detach()\n",
    "            fake_masked_data = get_interleaved_sequence_by_mask(generator_input_sequence, fake_data, get_mask_vector(sequence_length, eta))\n",
    "            \n",
    "            discriminator_decision_dg = D(fake_masked_data)\n",
    "            generator_error = criterion(discriminator_decision_dg, torch.ones(sequence_length))\n",
    "        \n",
    "            generator_error.backward()\n",
    "            generator_optimizer.step()\n",
    "            \n",
    "            ge = extract(generator_error)[0]\n",
    "            \n",
    "        if epoch % loss_log_interval == 0 or epoch == num_epochs-1:\n",
    "            discriminator_fake_losses.append(dfe)\n",
    "            generator_losses.append(ge)\n",
    "            \n",
    "        if print_stats:\n",
    "            if epoch % print_interval == 0 or epoch == num_epochs-1:\n",
    "                print(\"Epoch: \", epoch)\n",
    "                print(\"D Real Error: \", dre)\n",
    "                print(\"D Fake Error: \", dfe)\n",
    "                print(\"G Error: \", ge)\n",
    "            \n",
    "    return G, generator_losses, discriminator_fake_losses\n",
    "            \n",
    "#train on \"attack\" sequences\n",
    "gen, g_losses, df_losses = train(G,D,packet_data_df[packet_data_df['label'] == 1])\n",
    "plt.plot(range(len(df_losses)), df_losses, 'r-', range(len(g_losses)), g_losses, 'g-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode_feature_sequence_tensor(G(X_seq).detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
