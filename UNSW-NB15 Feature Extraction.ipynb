{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features file found!\n"
     ]
    }
   ],
   "source": [
    "#setup imports and make ure the files we care about exist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "features_path = \"/home/jaywalker/MachineLearning/UNSW-NB15-CSV/NUSW-NB15_features.csv\"\n",
    "print(\"Features file found!\" if os.path.isfile(\"/home/jaywalker/MachineLearning/UNSW-NB15-CSV/NUSW-NB15_features.csv\") else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['no.', 'name', 'type', 'description'], dtype='object')\n",
      "                name       type\n",
      "0              srcip    nominal\n",
      "1              sport    integer\n",
      "2              dstip    nominal\n",
      "3             dsport    integer\n",
      "4              proto    nominal\n",
      "5              state    nominal\n",
      "6                dur      float\n",
      "7             sbytes    integer\n",
      "8             dbytes    integer\n",
      "9               sttl    integer\n",
      "10              dttl    integer\n",
      "11             sloss    integer\n",
      "12             dloss    integer\n",
      "13           service    nominal\n",
      "14             sload      float\n",
      "15             dload      float\n",
      "16             spkts    integer\n",
      "17             dpkts    integer\n",
      "18              swin    integer\n",
      "19              dwin    integer\n",
      "20             stcpb    integer\n",
      "21             dtcpb    integer\n",
      "22           smeansz    integer\n",
      "23           dmeansz    integer\n",
      "24       trans_depth    integer\n",
      "25       res_bdy_len    integer\n",
      "26              sjit      float\n",
      "27              djit      float\n",
      "28             stime  timestamp\n",
      "29             ltime  timestamp\n",
      "30           sintpkt      float\n",
      "31           dintpkt      float\n",
      "32            tcprtt      float\n",
      "33            synack      float\n",
      "34            ackdat      float\n",
      "35   is_sm_ips_ports     binary\n",
      "36      ct_state_ttl    integer\n",
      "37  ct_flw_http_mthd    integer\n",
      "38      is_ftp_login     binary\n",
      "39        ct_ftp_cmd    integer\n",
      "40        ct_srv_src    integer\n",
      "41        ct_srv_dst    integer\n",
      "42        ct_dst_ltm    integer\n",
      "43       ct_src_ ltm    integer\n",
      "44  ct_src_dport_ltm    integer\n",
      "45  ct_dst_sport_ltm    integer\n",
      "46    ct_dst_src_ltm    integer\n",
      "47        attack_cat    nominal\n",
      "48             label     binary\n"
     ]
    }
   ],
   "source": [
    "#quick peek at the features and cleaning up column names for easier indexing\n",
    "features_df = pd.read_csv(features_path, encoding=\"latin-1\")\n",
    "for i in range(len(features_df.columns.values)):\n",
    "    features_df.columns.values[i] = str(features_df.columns.values[i]).strip().lower()\n",
    "    \n",
    "    \n",
    "print(features_df.columns) #cleaned up column names\n",
    "\n",
    "#lower case all the types\n",
    "for i in range(len(features_df)):\n",
    "    features_df.loc[i, ['type']] = str(features_df['type'][i]).strip().lower()\n",
    "    features_df.loc[i, ['name']] = str(features_df['name'][i]).strip().lower()\n",
    "\n",
    "print(features_df[['name', 'type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id       dur proto service state  spkts  dpkts  sbytes  dbytes        rate  \\\n",
      "0   1  0.000011   udp       -   INT      2      0     496       0  90909.0902   \n",
      "\n",
      "   ...    ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  \\\n",
      "0  ...                   1               2             0           0   \n",
      "\n",
      "   ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  attack_cat  \\\n",
      "0                 0           1           2                0      Normal   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "\n",
      "[1 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "#quick peek at the data\n",
    "training_set_path = \"/home/jaywalker/MachineLearning/UNSW-NB15-CSV/train_test/UNSW_NB15_training-set.csv\"\n",
    "training_df = pd.read_csv(training_set_path, encoding=\"latin-1\")\n",
    "print(training_df[:1])\n",
    "#Of COURSE this file is organized differently than the features file describes.\n",
    "#Why would I expect differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes',\n",
      "       'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service', 'sload', 'dload',\n",
      "       'spkts', 'dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz',\n",
      "       'dmeansz', 'trans_depth', 'res_bdy_len', 'sjit', 'djit', 'stime',\n",
      "       'ltime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
      "       'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login',\n",
      "       'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ ltm',\n",
      "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#so we'll use a file from the \"full\" dataset instead\n",
    "nb15_1_path = \"/home/jaywalker/MachineLearning/PacketGAN/UNSW-NB15_1_clean.csv\"\n",
    "packet_data_df = pd.read_csv(nb15_1_path, encoding=\"latin-1\", names=features_df['name'], header=None)\n",
    "print(packet_data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcip: ['175.45.176.0', '175.45.176.1', '175.45.176.2', '175.45.176.3'] type: nominal\n",
      "sport: 9607 type: integer\n",
      "dstip: ['149.171.126.15', '149.171.126.14', '149.171.126.10', '149.171.126.13', '149.171.126.18', '149.171.126.19', '149.171.126.11', '149.171.126.17', '149.171.126.12', '149.171.126.16'] type: nominal\n",
      "dsport: 786 type: integer\n",
      "proto: 129 type: nominal\n",
      "state: ['INT', 'CON', 'REQ', 'FIN', 'ACC', 'CLO'] type: nominal\n",
      "dur: 8748 type: float\n",
      "sbytes: 2604 type: integer\n",
      "dbytes: 2410 type: integer\n",
      "sttl: [254, 0, 62, 255, 63] type: integer\n",
      "dttl: [0, 60, 252, 253] type: integer\n",
      "sloss: 186 type: integer\n",
      "dloss: 200 type: integer\n",
      "service: ['-', 'dhcp', 'dns', 'http', 'ftp-data', 'smtp', 'pop3', 'ftp', 'snmp', 'radius', 'ssl', 'ssh', 'irc'] type: nominal\n",
      "sload: 9501 type: float\n",
      "dload: 8654 type: float\n",
      "spkts: 225 type: integer\n",
      "dpkts: 245 type: integer\n",
      "swin: [0, 255] type: integer\n",
      "dwin: [0, 255] type: integer\n",
      "stcpb: 8584 type: integer\n",
      "dtcpb: 8552 type: integer\n",
      "smeansz: 1072 type: integer\n",
      "dmeansz: 986 type: integer\n",
      "trans_depth: [0, 1, 2, 8, 4, 3] type: integer\n",
      "res_bdy_len: 492 type: integer\n",
      "sjit: 8678 type: float\n",
      "djit: 8552 type: float\n",
      "stime: 6490 type: timestamp\n",
      "ltime: 6081 type: timestamp\n",
      "sintpkt: 8719 type: float\n",
      "dintpkt: 8581 type: float\n",
      "tcprtt: 8247 type: float\n",
      "synack: 8110 type: float\n",
      "ackdat: 7970 type: float\n",
      "is_sm_ips_ports: [0] type: binary\n",
      "ct_state_ttl: [2, 3, 6, 1, 0, 4, 5] type: integer\n",
      "ct_flw_http_mthd: [0, 1, 2] type: integer\n",
      "is_ftp_login: [0, 1] type: binary\n",
      "ct_ftp_cmd: [0, 1] type: integer\n",
      "ct_srv_src: 33 type: integer\n",
      "ct_srv_dst: 31 type: integer\n",
      "ct_dst_ltm: [2, 3, 1, 4, 6, 5, 14, 7, 8, 9, 11, 16, 10, 12, 13, 15] type: integer\n",
      "ct_src_ ltm: [1, 3, 4, 2, 5, 15, 6, 7, 8, 9, 13, 11, 10, 12, 14, 16, 17] type: integer\n",
      "ct_src_dport_ltm: [1, 2, 5, 3, 4, 14, 6, 8, 7, 11, 15, 10, 16, 12, 9, 13] type: integer\n",
      "ct_dst_sport_ltm: [1, 2, 4, 14, 6, 8, 3, 5] type: integer\n",
      "ct_dst_src_ltm: [1, 2, 3, 4, 14, 6, 8, 5, 7, 11, 9, 16, 10, 12, 13, 15] type: integer\n",
      "attack_cat: ['Reconnaissance', 'Exploits', 'Generic', ' Fuzzers', 'DoS', 'Backdoors', 'Analysis', 'Shellcode', 'Worms'] type: nominal\n",
      "label: [1] type: binary\n",
      "Normal packets:  677786\n",
      "Attack packets:  22215\n"
     ]
    }
   ],
   "source": [
    "#for each feature of type \"nominal\" or \"integer\" count how many classes exist\n",
    "#print(packet_data_df['label'].unique()) #identify the different values\n",
    "\n",
    "for label, feature_type in features_df[['name', 'type']].values:\n",
    "    nunique = packet_data_df[packet_data_df['label'] == 1][label].nunique()\n",
    "    if nunique < 20:\n",
    "        value_list = packet_data_df[packet_data_df['label'] == 1][label].unique().tolist()\n",
    "        print(label + \": \" , end='')\n",
    "        print(value_list, end='')\n",
    "        print(\" type: \" + str(feature_type))\n",
    "    else:\n",
    "        print(label + \": \" + str(nunique) + \" type: \" + str(feature_type))\n",
    "        \n",
    "        \n",
    "#how many attack packets do we have compared to non-attack packets?\n",
    "print(\"Normal packets: \", len(packet_data_df[packet_data_df['label'] == 0].index))\n",
    "print(\"Attack packets: \", len(packet_data_df[packet_data_df['label'] == 1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the maximum port value?\n",
    "#attack_sports = packet_data_df[packet_data_df['label'] == 1]['sport']\n",
    "for k in range(0, packet_data_df['sport'].shape[0]):\n",
    "    if (isinstance(packet_data_df.loc[k, 'sport'], str)):\n",
    "        packet_data_df.loc[k, 'sport'] = int(packet_data_df.loc[k, 'sport'])\n",
    "        \n",
    "#did we convert all the strings?\n",
    "for k in range(0, packet_data_df['sport'].shape[0]):\n",
    "    if (isinstance(packet_data_df.loc[k, 'sport'], str)):\n",
    "        print(packet_data_df.loc[k, 'sport'])\n",
    "        \n",
    "#print(packet_data_df.loc[k, 'sport'])\n",
    "#sorted_ports = attack_sports.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#How can we encode these various features, many of which are discrete integers?\n",
    "#One-hot or Binary encoding seems logical, using Binary coding to keep things compact.\n",
    "\n",
    "#Returns a list where each element are a 1 or 0, determining the binary encoding of value with\n",
    "#at least bits number of bits. If the value cannot be encoding with the requested number of bits,\n",
    "#None will be returned.\n",
    "def binary_encode(value, bits):\n",
    "    encoding = []\n",
    "    while value != 0:\n",
    "        encoding.append(value % 2)\n",
    "        value //= 2\n",
    "        \n",
    "    if bits < len(encoding):\n",
    "        return None #couldn't represent with requested number of bits\n",
    "    \n",
    "    while len(encoding) < bits:\n",
    "        encoding.append(0)\n",
    "    \n",
    "    encoding.reverse()\n",
    "    return encoding\n",
    "        \n",
    "print(binary_encode(7, 4)) #returns [0,1,1,1]\n",
    "print(binary_encode(255, 2)) #returns None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#try converting an ip into a bit array\n",
    "ip_as_bits = []\n",
    "for byte in packet_data_df['srcip'][0].split('.'):\n",
    "    ip_as_bits += binary_encode(int(byte), 8)\n",
    "    \n",
    "print(ip_as_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1421927415\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#lets see what some of the other relevant fields look like\n",
    "timestamp = packet_data_df['stime'][0]\n",
    "print(timestamp)\n",
    "nbits = 36\n",
    "print(binary_encode(timestamp, nbits))\n",
    "#can all the timestamps be represented with fewer bits?\n",
    "for k in packet_data_df['stime']:\n",
    "    if binary_encode(k, nbits) is None:\n",
    "        print(\"Couldn't map all the timestamps!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name       type\n",
      "0              srcip    nominal\n",
      "1              sport    integer\n",
      "2              dstip    nominal\n",
      "3             dsport    integer\n",
      "4              proto    nominal\n",
      "5              state    nominal\n",
      "6                dur      float\n",
      "7             sbytes    integer\n",
      "8             dbytes    integer\n",
      "9               sttl    integer\n",
      "10              dttl    integer\n",
      "11             sloss    integer\n",
      "12             dloss    integer\n",
      "13           service    nominal\n",
      "14             sload      float\n",
      "15             dload      float\n",
      "16             spkts    integer\n",
      "17             dpkts    integer\n",
      "18              swin    integer\n",
      "19              dwin    integer\n",
      "20             stcpb    integer\n",
      "21             dtcpb    integer\n",
      "22           smeansz    integer\n",
      "23           dmeansz    integer\n",
      "24       trans_depth    integer\n",
      "25       res_bdy_len    integer\n",
      "26              sjit      float\n",
      "27              djit      float\n",
      "28             stime  timestamp\n",
      "29             ltime  timestamp\n",
      "30           sintpkt      float\n",
      "31           dintpkt      float\n",
      "32            tcprtt      float\n",
      "33            synack      float\n",
      "34            ackdat      float\n",
      "35   is_sm_ips_ports     binary\n",
      "36      ct_state_ttl    integer\n",
      "37  ct_flw_http_mthd    integer\n",
      "38      is_ftp_login     binary\n"
     ]
    }
   ],
   "source": [
    "#what features do I care about?\n",
    "#all the non-aggregate features that are some combination of the other features\n",
    "#either directly in an example or temporal combinations, since these should ostensibly\n",
    "#be discovered by the GAN\n",
    "features_to_use = features_df[:39]\n",
    "print(features_to_use[['name', 'type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature encoding\n",
    "-------------------------\n",
    "\n",
    "I will encode the integer based features using a binary representation, using the minimum number of bits to represent the max value plus one bit. Float based parameters will be scaled in a typical manner.\n",
    "\n",
    "IP addresses in particular are a special case, since each field is represending a collection of 4 bytes. These addresses will be represented as 32 bits, since this is the native representation and seems appropriate for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([96])\n",
      "torch.Size([5, 1, 96])\n",
      "tensor([[[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "def get_minimum_bits(value):\n",
    "    min_bits = 1\n",
    "    while binary_encode(value, min_bits) is None:\n",
    "        min_bits += 1\n",
    "        \n",
    "    return min_bits\n",
    "\n",
    "def build_input_feature_tensor(packet_data_dict):\n",
    "    input_features = []\n",
    "    \n",
    "    srcip_segments = str(packet_data_dict['srcip']).split('.')\n",
    "    srcip_bits = []\n",
    "    for segment in srcip_segments:\n",
    "        for k in binary_encode(int(segment), 8):\n",
    "            srcip_bits.append(k)\n",
    "    \n",
    "    dstip_segments = str(packet_data_dict['dstip']).split('.')\n",
    "    dstip_bits = []\n",
    "    for segment in dstip_segments:\n",
    "        for k in binary_encode(int(segment), 8):\n",
    "            dstip_bits.append(k)\n",
    "            \n",
    "    sport = binary_encode(int(packet_data_dict['sport']), 16)#get_minimum_bits(int(packet_data_dict['sport'])) + 1)\n",
    "    dport = binary_encode(int(packet_data_dict['dsport']), 16)#get_minimum_bits(int(packet_data_dict['dsport'])) + 1)\n",
    "    \n",
    "    #TODO need to encode the rest of the features buuuuuttttt that can come later.\n",
    "    \n",
    "    input_features += srcip_bits + dstip_bits + sport + dport\n",
    "    \n",
    "    return torch.tensor(input_features, dtype=torch.float32)\n",
    "        \n",
    "X = build_input_feature_tensor(packet_data_df.loc[0,:].to_dict())\n",
    "\n",
    "#just playing with tensors here to figure out what I'm doing!\n",
    "print(X)\n",
    "print(X.shape)\n",
    "X_seq = torch.tensor(()).new_zeros([5,1,X.shape[0]])\n",
    "print(X_seq.shape)\n",
    "\n",
    "#wow, I did not expect this to work!\n",
    "X_seq[:,0,:] = X\n",
    "#print(X_seq)\n",
    "\n",
    "def build_input_sequence_tensor(packet_data_df, sequence_length):\n",
    "    example_feature_vector = build_input_feature_tensor(packet_data_df.loc[0,:].to_dict())\n",
    "    seq_out = torch.tensor(()).new_zeros([sequence_length, 1, example_feature_vector.shape[0]])\n",
    "    \n",
    "    for i in range(0, sequence_length):\n",
    "        #print(seq_out.shape)\n",
    "        seq_out[i,0,:] = build_input_feature_tensor(packet_data_df.loc[i,:].to_dict())\n",
    "        \n",
    "    return seq_out\n",
    "\n",
    "X_seq = build_input_sequence_tensor(packet_data_df, 5)\n",
    "print(X_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I shouldn't actually do the training in this notebook, this is mostly a test to see if I've prepared\n",
    "#the features correctly for input to some RNN network.\n",
    "\n",
    "#MODELS: Define Generator model and Discriminator model\n",
    "#For the time being, this will be a one-layer RNN that is the same width as the input feature tensor\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h0 = torch.zeros(1,1,hidden_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, self.h0 = self.gru(x, self.h0)\n",
    "        return self.f(x)\n",
    "    \n",
    "    def reset_hidden_state(self):\n",
    "        self.h0 = torch.zeros(1,1,self.hidden_size)\n",
    "\n",
    "#discriminator is copy of the generator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h0 = torch.zeros(1,1,hidden_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.map = nn.Linear(input_size, 1) # + hidden_size, 1)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, self.h0 = self.gru(x, self.h0)\n",
    "        x = self.map(x) #torch.cat((x, self.h0),1))\n",
    "        return self.f(x)\n",
    "    \n",
    "    def reset_hidden_state(self):\n",
    "        self.h0 = torch.zeros(1,1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/jaywalker/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#Define Generator and Discriminator\n",
    "G = Generator(X_seq.shape[2], X_seq.shape[2], X_seq.shape[2], torch.sigmoid)\n",
    "D = Discriminator(X_seq.shape[2], X_seq.shape[2], 1, torch.sigmoid)\n",
    "\n",
    "#testing how resetting the hidden state has an effect on the output\n",
    "G.zero_grad()\n",
    "G.reset_hidden_state()\n",
    "first = G(torch.tensor(X_seq).clone())\n",
    "#print(first)\n",
    "\n",
    "G.zero_grad()\n",
    "G.reset_hidden_state()\n",
    "second = G(torch.tensor(X_seq).clone())\n",
    "#print(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator input shape:  torch.Size([3, 1, 96])\n",
      "Fake data element shape:  torch.Size([3, 1, 96])\n",
      "Fake data shape:  torch.Size([4, 1, 96])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-0f4f46c62ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-0f4f46c62ebd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(G, D, real_data)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdiscriminator_decision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdiscriminator_fake_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_decision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mdiscriminator_fake_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
    "\n",
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "#well OK let's train the GAN on a single sequence and see what happens\n",
    "def train(G, D, real_data):\n",
    "    num_epochs = 1\n",
    "    print_interval = 1\n",
    "    \n",
    "    discriminator_learning_rate = 1e-3\n",
    "    generator_learning_rate = 1e-3\n",
    "    sgd_momentum = 0.9\n",
    "    \n",
    "    discriminator_training_steps = 1\n",
    "    generator_training_steps = 1\n",
    "    \n",
    "    discriminator_fake_error, discriminator_real_error = 0, 0\n",
    "    generator_error = 0\n",
    "    \n",
    "    criterion = nn.BCELoss() #right now the output is binary so this makes sense\n",
    "    discriminator_optimizer = optim.SGD(D.parameters(), lr=discriminator_learning_rate, momentum=sgd_momentum)\n",
    "    generator_optimizer = optim.SGD(G.parameters(), lr=discriminator_learning_rate, momentum=sgd_momentum)\n",
    "    \n",
    "    sequence_length = real_data.shape[0]\n",
    "    for epoch in range(num_epochs):\n",
    "        for discriminator_step in range(discriminator_training_steps):\n",
    "            D.reset_hidden_state()\n",
    "            D.zero_grad()\n",
    "            \n",
    "            #Train D on the real samples\n",
    "            #print(real_data.shape)\n",
    "            discriminator_decision = D(real_data)\n",
    "            #print(discriminator_decision.shape)\n",
    "            discriminator_real_error = criterion(discriminator_decision, torch.ones(sequence_length))\n",
    "            discriminator_real_error.backward()\n",
    "            \n",
    "            #Train D on the fake samples\n",
    "            \n",
    "            #create a sample of length-1 real samples and generate the last\n",
    "            generator_input = real_data[:sequence_length - 2, :, :]\n",
    "            print(\"Generator input shape: \", generator_input.shape)\n",
    "            fake_data = torch.zeros(sequence_length - 1, 1, real_data.shape[2])\n",
    "            fake_data[:sequence_length - 2, :, :] = generator_input\n",
    "            \n",
    "            fake_data_element = G(generator_input).clone()\n",
    "            print(\"Fake data element shape: \", fake_data_element.shape)\n",
    "            print(\"Fake data shape: \", fake_data.shape)\n",
    "            fake_data[sequence_length - 2, :, :] = fake_data_element[2, :, :] #generate the last packet in the sequence\n",
    "            discriminator_decision = D(fake_data)\n",
    "            discriminator_fake_error = criterion(discriminator_decision, torch.zeros(sequence_length - 1))\n",
    "            discriminator_fake_error.backward()\n",
    "            discriminator_optimizer.step()\n",
    "            \n",
    "            dre = extract(discriminator_real_error)[0]\n",
    "            dfe = extract(discriminator_fake_error)[0]\n",
    "    \n",
    "        for generator_step in range(generator_training_steps):\n",
    "            print(\"generator step stub\")\n",
    "            \n",
    "            \n",
    "        if epoch % print_interval == 0 or epock == num_epochs-1:\n",
    "            print(\"D Real Error: \", dre)\n",
    "            print(\"D Fake Error: \", dfe)\n",
    "            \n",
    "            \n",
    "train(G,D,X_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
